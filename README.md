# Sequoia - Cliente Spark para AWS Glue

Uma biblioteca Python simplificada para trabalhar com Apache Spark no AWS Glue, oferecendo uma interface limpa e eficiente para operaÃ§Ãµes de dados.

## ğŸš€ CaracterÃ­sticas Principais

- **Singleton Pattern**: InstÃ¢ncia Ãºnica do cliente
- **ConfiguraÃ§Ã£o Otimizada**: Spark configurado automaticamente
- **Leitura Simplificada**: Um mÃ©todo para todos os formatos
- **Escrita FlexÃ­vel**: Suporte a mÃºltiplos formatos
- **Logging Integrado**: Sistema de logs robusto
- **Tratamento de Erros**: Gerenciamento de exceÃ§Ãµes

## ğŸ“ Estrutura do Projeto

```
sequoia/
â”œâ”€â”€ sequoia/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py          # Cliente principal
â”‚   â””â”€â”€ logger.py        # Sistema de logs
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ config_examples.py
â”‚   â”œâ”€â”€ file_operations_example.py
â”‚   â”œâ”€â”€ singleton_example.py
â”‚   â”œâ”€â”€ sql_join_example.py
â”‚   â””â”€â”€ union_example.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaÃ§Ã£o

```bash
pip install -r requirements.txt
```

## ğŸ“– Uso BÃ¡sico

```python
from sequoia import GlueClient

# Inicializar cliente
sq = GlueClient()

# Ler tabela (funciona com qualquer formato)
df = sq.read_table("database", "table")

# Ler com filtros
df = sq.read_table(
    "database", 
    "table",
    columns=["id", "nome", "data"],
    where="data >= '2024-01-01'"
)

# Escrever arquivo
sq.write_file(df, "s3://bucket/dados.parquet")

# Executar SQL
df = sq.sql("SELECT * FROM database.table WHERE id > 100")
```

## ğŸ”§ MÃ©todos Principais

### Leitura de Dados
- **`read_table()`**: LÃª tabelas do catÃ¡logo (qualquer formato)
- **`read_file()`**: LÃª arquivos (Parquet, CSV, Excel)

### Escrita de Dados
- **`write_file()`**: Escreve DataFrames em arquivos
- **`write_table()`**: Escreve DataFrames como tabelas

### ConfiguraÃ§Ã£o
- **`update_config()`**: Atualiza configuraÃ§Ãµes Spark
- **`get_current_config()`**: ObtÃ©m configuraÃ§Ãµes atuais

### Job Management
- **`job_init()`**: Inicializa job Glue
- **`job_commit()`**: Finaliza job Glue

### UtilitÃ¡rios
- **`sql()`**: Executa queries SQL
- **`createDataFrame()`**: Cria DataFrames
- **`get_table_info()`**: ObtÃ©m informaÃ§Ãµes da tabela

## ğŸ¯ Leitura Simplificada

O mÃ©todo `read_table()` funciona automaticamente com qualquer formato:

```python
# Funciona com Iceberg
df_iceberg = sq.read_table("database", "iceberg_table")

# Funciona com Parquet
df_parquet = sq.read_table("database", "parquet_table")

# Funciona com CSV
df_csv = sq.read_table("database", "csv_table")
```

## âš™ï¸ ConfiguraÃ§Ã£o Otimizada do Spark

O Sequoia inicializa o Spark com configuraÃ§Ãµes otimizadas:

```python
# ConfiguraÃ§Ãµes automÃ¡ticas incluÃ­das:
# - KryoSerializer para performance
# - ConfiguraÃ§Ãµes Parquet otimizadas
# - ConfiguraÃ§Ãµes de memÃ³ria e particionamento
# - ConfiguraÃ§Ãµes de timezone
# - ConfiguraÃ§Ãµes de checkpoint

# Adicionar configuraÃ§Ãµes customizadas
sq.update_config({
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
})
```

## ğŸ“ Exemplos

### Exemplo 1: Leitura Simples
```python
from sequoia import GlueClient

sq = GlueClient()
df = sq.read_table("meu_database", "minha_tabela")
print(f"Total de registros: {df.count()}")
```

### Exemplo 2: Leitura com Filtros
```python
df = sq.read_table(
    "meu_database", 
    "minha_tabela",
    columns=["id", "nome", "data"],
    where="data >= '2024-01-01'"
)
```

### Exemplo 3: Escrita de Arquivos
```python
# Parquet (padrÃ£o)
sq.write_file(df, "s3://bucket/dados.parquet")

# CSV
sq.write_file(df, "s3://bucket/dados.csv", format_type="csv")

# Com opÃ§Ãµes customizadas
sq.write_file(
    df, 
    "s3://bucket/dados.parquet",
    options={"compression": "gzip"}
)
```

### Exemplo 4: Queries SQL
```python
df = sq.sql("""
    SELECT 
        data,
        COUNT(*) as total
    FROM meu_database.minha_tabela
    WHERE data >= '2024-01-01'
    GROUP BY data
    ORDER BY data
""")
```

## ğŸ” Tratamento de Erros

O Sequoia inclui tratamento robusto de erros:

```python
try:
    df = sq.read_table("database", "table")
except Exception as e:
    print(f"Erro ao ler tabela: {str(e)}")
```

## ğŸ“Š Logging

Sistema de logs integrado para debugging:

```python
# Logs automÃ¡ticos para todas as operaÃ§Ãµes
# NÃ­veis: INFO, WARNING, ERROR
```

## ğŸš€ Performance

- **ConfiguraÃ§Ãµes Otimizadas**: Spark configurado para performance
- **Singleton Pattern**: Evita mÃºltiplas instÃ¢ncias
- **Leitura Eficiente**: OtimizaÃ§Ãµes automÃ¡ticas
- **MemÃ³ria Gerenciada**: ConfiguraÃ§Ãµes de memÃ³ria otimizadas

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.