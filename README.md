# Sequoia

Biblioteca Python para desenvolvimento de jobs AWS Glue 5 com otimiza√ß√µes KryoSerializer e suporte a leitura de dados Parquet e Iceberg. A classe `GlueClient` encapsula e simplifica o uso dos contextos Spark.

## Estrutura do Projeto

```
sequoia/
‚îú‚îÄ‚îÄ main.py                 # Job principal de exemplo
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o
‚îú‚îÄ‚îÄ sequoia/
‚îÇ   ‚îú‚îÄ‚îÄ core.py             # Classe principal GlueClient (Singleton)
‚îÇ   ‚îú‚îÄ‚îÄ logger.py           # Logger customizado
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ simple_auto_detection.py    # Detec√ß√£o autom√°tica
‚îÇ   ‚îú‚îÄ‚îÄ iceberg_example.py          # Exemplo Iceberg
‚îÇ   ‚îú‚îÄ‚îÄ sql_join_example.py         # Exemplo SQL
‚îÇ   ‚îú‚îÄ‚îÄ config_examples.py          # Configura√ß√µes customizadas
‚îÇ   ‚îú‚îÄ‚îÄ optimized_config_example.py # Configura√ß√£o otimizada
‚îÇ   ‚îú‚îÄ‚îÄ config_error_handling_example.py  # Tratamento de erros de config
‚îÇ   ‚îú‚îÄ‚îÄ s3_permission_example.py    # Tratamento de permiss√µes S3
‚îÇ   ‚îú‚îÄ‚îÄ file_operations_example.py  # Opera√ß√µes com arquivos
‚îÇ   ‚îú‚îÄ‚îÄ singleton_example.py        # Padr√£o Singleton
‚îÇ   ‚îú‚îÄ‚îÄ union_example.py            # UNION entre tabelas
‚îÇ   ‚îî‚îÄ‚îÄ iceberg_detection_example.py  # Detec√ß√£o de Iceberg
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ index.html          # Documenta√ß√£o HTML
```

## Configura√ß√µes KryoSerializer

O projeto utiliza o KryoSerializer para otimizar o processamento de dados Parquet:

### Configura√ß√µes Aplicadas

- **KryoSerializer**: Serializa√ß√£o otimizada para melhor performance
- **Compress√£o Snappy**: Para arquivos Parquet
- **Vectorized Reader**: Leitura otimizada de Parquet
- **Adaptive Query Execution**: Otimiza√ß√µes autom√°ticas do Spark
- **Configura√ß√µes de Mem√≥ria**: Buffer e cache otimizados

### Benef√≠cios

- Melhor performance na serializa√ß√£o
- Redu√ß√£o do uso de mem√≥ria
- Otimiza√ß√µes autom√°ticas de queries
- Leitura/escrita otimizada de Parquet
- Suporte a leitura de Iceberg (time travel e snapshots)

## Como Usar

### 1. Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### 2. Configurar Par√¢metros do Job

O job aceita os seguintes par√¢metros:

- `JOB_NAME`: Nome do job
- `database_name`: Nome do banco de dados no Glue Catalog
- `table_name`: Nome da tabela (Parquet ou Iceberg)
- `output_path`: Caminho S3 para salvar resultados (opcional)

### 3. Executar o Job

```bash
python main.py \
  --JOB_NAME=meu-job \
  --database_name=meu_database \
  --table_name=minha_tabela \
  --output_path=s3://bucket/output/
```

### 4. Detec√ß√£o Autom√°tica de Iceberg

O job detecta automaticamente se a tabela √© Iceberg ou formato padr√£o:

```python
# M√©todo autom√°tico (RECOMENDADO)
df = sq.read_table("database", "table")

# M√©todo manual (para debug)
table_format = sq.detect_table_format("database", "table")
if table_format == "iceberg":
    df = sq.read_iceberg_table_from_catalog("database", "table")
else:
    df = sq.read_table_from_catalog("database", "table")
```

## Funcionalidades

### Leitura de Dados

- Leitura de tabelas do Glue Catalog (Parquet, Iceberg)
- **Detec√ß√£o autom√°tica de Iceberg** - 100-1000x mais r√°pida
- Leitura direta de arquivos Parquet do S3
- Otimiza√ß√µes autom√°ticas com KryoSerializer

### Processamento

- An√°lise de schema dos dados
- Contagem de registros
- Agrega√ß√µes b√°sicas
- Queries SQL com JOINs
- Cria√ß√£o de DataFrames

### Escrita de Dados

- Escrita otimizada em formato Parquet
- Configura√ß√µes de compress√£o e particionamento
- Op√ß√µes de modo de escrita (overwrite, append, error, ignore)
- Suporte a leitura de tabelas Iceberg (time travel e snapshots)

## Utilit√°rios

### Sequoia

Classe principal que encapsula SparkContext, GlueContext e SparkSession:

**Inicializa√ß√£o Simplificada:**
```python
# Cria automaticamente todos os contextos
sq = GlueClient()

# Ou com contextos espec√≠ficos (opcional)
sq = GlueClient(spark_context=sc, glue_context=glue_context, spark_session=spark)
```

**M√©todos Principais:**
- **Leitura autom√°tica**: `read_table()` - detecta Iceberg e l√™ automaticamente
- **Detec√ß√£o de formato**: `detect_table_format()` - identifica Iceberg via metadados
- **Leitura espec√≠fica**: `read_table_from_catalog()` e `read_iceberg_table_from_catalog()`
- **Leitura de arquivos**: `read_file()` - l√™ Parquet, CSV, Excel
- **Escrita de arquivos**: `write_file()` - escreve Parquet, CSV, Excel
- **Escrita de tabelas**: Apenas Parquet
- **Otimiza√ß√µes de DataFrame**
- **Informa√ß√µes sobre tabelas**

- **Controle de Job**: `job_init()`, `job_commit()` - para job bookmarks
- **Argumentos**: `get_job_args()`, `get_arg()` - encapsulamento de argumentos

**Acesso aos Contextos:**
```python
sequoia.spark_context    # SparkContext
sequoia.glue_context     # GlueContext  
sequoia.spark_session    # SparkSession
```

### Logger

Logger customizado para:
- Logs estruturados
- Diferentes n√≠veis de log
- Formata√ß√£o consistente

## Exemplo de Uso

```python
from sequoia.core import GlueClient

# Criar utilit√°rios (contextos criados automaticamente)
sq = GlueClient()

# M√©todo 1: Detec√ß√£o autom√°tica (RECOMENDADO)
df = sq.read_table("database", "table")

# M√©todo 2: Detec√ß√£o manual (para debug)
table_format = sq.detect_table_format("database", "table")
if table_format == "iceberg":
    df = sq.read_iceberg_table_from_catalog("database", "table")
else:
    df = sq.read_table_from_catalog("database", "table")

# Processar dados
df_processed = df  # DataFrame processado

# Salvar resultados (Parquet ou Iceberg)
sq.write_file(df_processed, "s3://bucket/output/")
# Ou para tabela:
sq.write_table(df_processed, "database", "table")
```

## Opera√ß√µes com Arquivos

O Sequoia suporta leitura e escrita de diferentes formatos de arquivo:

### Leitura de Arquivos

```python
# Ler Parquet (padr√£o)
df = sequoia.read_file("s3://bucket/data.parquet")

# Ler CSV
df = sequoia.read_file(
    "s3://bucket/data.csv",
    format_type="csv",
    options={
        "header": "true",
        "delimiter": ",",
        "encoding": "UTF-8"
    }
)

# Ler Excel
df = sequoia.read_file(
    "s3://bucket/data.xlsx",
    format_type="excel"
)

# Ler Excel com planilha espec√≠fica
df = sequoia.read_file(
    "s3://bucket/data.xlsx",
    format_type="excel",
    options={
        "sheet_name": "Planilha1"
    }
)

# Ler Excel com √≠ndice de planilha
df = sequoia.read_file(
    "s3://bucket/data.xlsx",
    format_type="excel",
    options={
        "sheet_name": 1  # Segunda planilha
    }
)

# Ler Parquet com op√ß√µes customizadas
df = sequoia.read_file(
    "s3://bucket/data.parquet",
    format_type="parquet",
    options={
        "compression": "gzip",
        "mergeSchema": "true"
    }
)
```

### Escrita de Arquivos

```python
# Salvar como Parquet
sequoia.write_file(df, "s3://bucket/output.parquet")

# Salvar como CSV
sequoia.write_file(
    df,
    "s3://bucket/output.csv",
    format_type="csv",
    options={
        "header": "true",
        "delimiter": ","
    }
)

# Salvar como Parquet com compress√£o gzip
sequoia.write_file(
    df,
    "s3://bucket/output.parquet",
    format_type="parquet",
    options={
        "compression": "gzip"
    }
)
```

**Nota**: Atualmente suporta apenas Parquet e CSV. Para arquivos Excel, use o m√©todo anterior.

### Op√ß√µes por Formato

#### **CSV Options (Leitura):**
```python
options = {
    "header": "true",           # Se tem cabe√ßalho
    "delimiter": ",",           # Delimitador
    "inferSchema": "true",      # Inferir schema
    "encoding": "UTF-8"         # Encoding
}
```

#### **CSV Options (Escrita):**
```python
options = {
    "header": "true",           # Incluir cabe√ßalho
    "delimiter": ",",           # Delimitador
    "encoding": "UTF-8"         # Encoding
}
```

#### **Parquet Options (Leitura):**
```python
options = {
    "compression": "snappy",    # Compress√£o
    "mergeSchema": "false",     # Mesclar schemas
    "columnarReaderBatchSize": "4096"  # Tamanho do batch
}
```

#### **Parquet Options (Escrita):**
```python
options = {
    "compression": "snappy"     # Compress√£o (snappy, gzip, zstd)
}
```

#### **Excel Options (Leitura):**
```python
options = {
    "sheet_name": 0,           # Nome ou √≠ndice da planilha (0 = primeira)
    "header": 0,               # Linha do cabe√ßalho (0 = primeira linha)
    "engine": "openpyxl"       # Engine para ler Excel
}
```

## Job Bookmarks

O Sequoia suporta controle de job para job bookmarks:

```python
# Inicializar job (necess√°rio para job bookmarks)
sequoia.job_init("meu_job", args)

# Ler dados normalmente
df = sequoia.read_table("database", "table")

# Finalizar job (necess√°rio para job bookmarks)
sequoia.job_commit()
```

## Controle de Job

Para jobs que usam job bookmarks, voc√™ pode controlar a inicializa√ß√£o e finaliza√ß√£o:

```python
# Inicializar job (necess√°rio para job bookmarks)
sequoia.job_init("meu_job", args)

# ... seu c√≥digo aqui ...

# Finalizar job (necess√°rio para job bookmarks)
sequoia.job_commit()
```

**Nota**: Esses m√©todos s√£o opcionais e s√≥ s√£o necess√°rios se voc√™ estiver usando job bookmarks.

## Configura√ß√µes de Parquet

O Sequoia inclui configura√ß√µes otimizadas para Parquet que s√£o aplicadas automaticamente pelo SparkSession:

```python
# Configura√ß√µes autom√°ticas aplicadas no SparkSession:
# - spark.sql.parquet.block.size: 134217728 (128MB)
# - spark.sql.parquet.page.size: 1048576 (1MB)
# - spark.sql.parquet.compression.codec: snappy
# - spark.sql.parquet.enableVectorizedReader: true

# As configura√ß√µes s√£o aplicadas automaticamente em todas as opera√ß√µes:
# - write_file()
# - write_table()
```

**Nota**: Como as configura√ß√µes s√£o aplicadas globalmente no SparkSession, n√£o √© necess√°rio especificar `block.size` e `page.size` individualmente em cada opera√ß√£o de escrita.

## Argumentos do Job

O Sequoia encapsula automaticamente os argumentos do job:

```python
# Obter todos os argumentos
args = sequoia.get_job_args(["JOB_NAME", "database_name", "table_name"])

# Obter argumento espec√≠fico
database_name = sequoia.get_arg("database_name")
table_name = sequoia.get_arg("table_name", default="minha_tabela")

# Inicializar job com argumentos autom√°ticos
sequoia.job_init()  # Obt√©m argumentos automaticamente
```

## Detec√ß√£o de Iceberg via Metadados

O Sequoia implementa detec√ß√£o de tabelas Iceberg ultra-r√°pida usando apenas metadados do Glue Catalog:

### Como Funciona

1. **Consulta aos metadados**: Acessa apenas informa√ß√µes do Glue Catalog
2. **An√°lise de propriedades**: Verifica par√¢metros espec√≠ficos do Iceberg
3. **Detec√ß√£o espec√≠fica**: Identifica se √© tabela Iceberg ou formato padr√£o
4. **Performance excepcional**: 100-1000x mais r√°pida que consultar dados

### Vantagens

- **‚ö° Ultra-r√°pida**: ~10-50ms vs ~1-10 segundos
- **üí∞ Baixo custo**: Apenas consulta ao Glue Catalog
- **üîç Precis√£o**: Detecta especificamente tabelas Iceberg
- **üéØ Simples**: Iceberg ou formato padr√£o

## Detec√ß√£o de Iceberg Otimizada

Com a detec√ß√£o via metadados sendo **100-1000x mais r√°pida**, n√£o h√° necessidade de cache:

### M√©todo de Detec√ß√£o

```python
# Detec√ß√£o autom√°tica de Iceberg via metadados (RECOMENDADO)
format = sq.detect_table_format("database", "table")

# Resultados poss√≠veis:
# - "iceberg" - Tabela Iceberg detectada
# - "standard" - Tabela padr√£o (Parquet, etc.)
```

### Exemplo de Performance

```python
# Detec√ß√£o via metadados (muito r√°pida)
df1 = sq.read_table("database", "table")  # ~10-50ms

# Detec√ß√£o direta (sempre r√°pida)
df2 = sq.read_table("database", "table")  # ~10-50ms

# Detec√ß√£o direta (sempre r√°pida)
df3 = sq.read_table("database", "table")  # ~10-50ms
```

**Nota**: Com detec√ß√£o via metadados t√£o r√°pida, n√£o h√° necessidade de cache.

## Configura√ß√£o Otimizada do Spark

O Sequoia implementa uma estrat√©gia robusta de configura√ß√£o via SparkConf:

### Configura√ß√µes Padr√£o Unificadas

```python
# Configura√ß√µes aplicadas automaticamente na inicializa√ß√£o:
default_config = {
    # KryoSerializer para performance
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryo.registrationRequired": "false",
    "spark.kryoserializer.buffer.max": "2047m",
    
    # Parquet otimizado
    "spark.sql.parquet.compression.codec": "snappy",
    "spark.sql.parquet.filterPushdown": "true",
    "spark.sql.parquet.block.size": "134217728",  # 128MB
    
    # Performance adaptativa
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
}
```

### Vantagens da Nova Estrat√©gia

- **‚úÖ Sem erros CANNOT_MODIFY_CONFIG**: Configura√ß√µes aplicadas na inicializa√ß√£o
- **‚úÖ Configura√ß√µes unificadas**: Todas em um local centralizado
- **‚úÖ Fallback robusto**: SparkContext padr√£o se falhar
- **‚úÖ Configura√ß√µes customizadas**: Suportadas via par√¢metro
- **‚úÖ Compatibilidade**: Funciona em AWS Glue e ambiente local
- **‚úÖ Tratamento de erro**: GlueContext com fallback para SparkSession

### Uso Otimizado

```python
# ‚úÖ Configura√ß√µes otimizadas aplicadas automaticamente
client = GlueClient()

# ‚úÖ Com configura√ß√µes customizadas
custom_config = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
}
client = GlueClient(spark_config=custom_config)

# ‚úÖ Verificar configura√ß√µes aplicadas
config = client.get_current_config()
```

## Tratamento de Erros S3

O Sequoia inclui diagn√≥stico autom√°tico para problemas de permiss√£o S3:

### Erro Comum: AccessDenied

```python
# Erro que pode ocorrer:
# org.apache.hadoop.fs.s3a.AWSS3IOException: 
# com.amazonaws.services.s3.model.AmazonS3Exception: 
# Access Denied (Service: Amazon S3; Status Code: 403)
```

### Diagn√≥stico Autom√°tico

O m√©todo `write_file()` inclui diagn√≥stico autom√°tico:

```python
# Tentativa de escrita com diagn√≥stico
try:
    client.write_file(df, "s3://meu-bucket/dados.parquet")
except Exception as e:
    # Diagn√≥stico autom√°tico √© logado
    print(f"Erro: {str(e)}")
```

### Permiss√µes IAM Necess√°rias

Para o IAM Role do Glue:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::meu-bucket",
        "arn:aws:s3:::meu-bucket/*"
      ]
    }
  ]
}
```

## Uso B√°sico

```python
from sequoia import GlueClient

# Criar inst√¢ncia (Singleton - sempre a mesma)
sq = GlueClient()

# Logs integrados
sq.logger.info("Iniciando processamento")
sq.logger.warning("Aviso importante")
sq.logger.error("Erro encontrado")

# Ler dados (detec√ß√£o autom√°tica de Iceberg)
df = sq.read_table("database", "table")

# Detec√ß√£o manual de formato
table_format = sq.detect_table_format("database", "table")
if table_format == "iceberg":
    df = sq.read_iceberg_table_from_catalog("database", "table")
else:
    df = sq.read_table_from_catalog("database", "table")
```

## Padr√£o Singleton

O Sequoia implementa o padr√£o Singleton, garantindo uma √∫nica inst√¢ncia:

```python
# Sempre a mesma inst√¢ncia
sq1 = Sequoia()
sq2 = Sequoia()
print(sq1 is sq2)  # True

# Logs integrados
sq.logger.info("Usando logs integrados")
sq.logger.warning("Aviso")
sq.logger.error("Erro")
```

### Vantagens do Singleton:

1. **Inst√¢ncia √önica**: Sempre a mesma inst√¢ncia em todo o c√≥digo
2. **Logs Integrados**: Acesso direto via `sq.logger.info()`
3. **Configura√ß√£o Centralizada**: Configura√ß√µes aplicadas uma vez
4. **Mem√≥ria Otimizada**: Evita m√∫ltiplas inst√¢ncias desnecess√°rias